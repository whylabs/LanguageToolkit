{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/whylabs/LanguageToolkit/blob/main/langkit/examples/Sentiment_and_Toxicity.ipynb)"
      ],
      "metadata": {
        "id": "vvANigl6Hrcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install LangKit\n",
        "\n",
        "First let's install __LangKit__."
      ],
      "metadata": {
        "id": "frxW2u3RJk1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langkit[all]"
      ],
      "metadata": {
        "id": "f5IsgjyuKGWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAeYfAYnHp-u"
      },
      "source": [
        "# Tracking Sentiment and Toxicity Scores in Text with Langkit\n",
        "\n",
        "In this example, we'll show how you can easily track sentiment and toxicity scores in text with Langkit.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px1lNOkHHp-w"
      },
      "source": [
        "As an example, we'll use the [tweet_eval dataset](https://huggingface.co/datasets/tweet_eval). We'll use the `hateful` subset of the dataset, which contains tweets labeled as hateful or not hateful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0YNDLPOHp-w"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "hateful_comments = load_dataset('tweet_eval','hate',split=\"train\", streaming=True)\n",
        "comments = iter(hateful_comments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beyHBap8Hp-x"
      },
      "source": [
        "## Initializing the Metrics\n",
        "\n",
        "To initialize the `toxicity` and `sentiment` metrics, we simply import the respective modules from `langkit`. This will automatically register the metrics, so we can start using them right away by creating a schema by calling `generate_udf_schema`. We will pass that schema to whylogs, so that it knows which metrics to track."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRHZrfT2Hp-y"
      },
      "outputs": [],
      "source": [
        "\n",
        "from whylogs.experimental.core.metrics.udf_metric import generate_udf_resolvers\n",
        "from whylogs.core.schema import DeclarativeSchema\n",
        "from langkit import toxicity, sentiment\n",
        "\n",
        "\n",
        "text_schema = DeclarativeSchema(generate_udf_resolvers())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_68Wn0lHp-y"
      },
      "source": [
        "## Profiling the Data\n",
        "\n",
        "Now we're set to log our data.\n",
        "\n",
        "To make sure the metrics make sense, we will profile two separate groups of data:\n",
        "- hateful comments: comments that are labeled as hateful\n",
        "- non-hateful comments: comments that are labeled as non-hateful\n",
        "\n",
        "We can expect hateful comments to have a higher toxicity score and a lower sentiment score than non-hateful comments.\n",
        "\n",
        "Let's see if our metrics will reflect that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9_nLmLE1Hp-z"
      },
      "outputs": [],
      "source": [
        "import whylogs as why\n",
        "\n",
        "# Just initializing the profiles with generic comments.\n",
        "non_hateful_profile = why.log({\"comment\":\"I love flowers.\"}, schema=text_schema).profile()\n",
        "hateful_profile = why.log({\"comment\":\"I hate biscuits.\"}, schema=text_schema).profile()\n",
        "\n",
        "for _ in range(200):\n",
        "  comment = next(comments)\n",
        "  if comment['label'] == 0:\n",
        "    non_hateful_profile.track({\"comment\":comment['text']})\n",
        "  else:\n",
        "    hateful_profile.track({\"comment\":comment['text']})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rhVn7ToHp-z"
      },
      "source": [
        "Now that we have our profiles, let's check out the metrics. Let's compare the mean for our sentiment and toxicity scores, for each group (hateful and non-hateful):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "43mdtqyIHp-0",
        "outputId": "99bc1546-8313-4517-e2a4-aa3241a07226",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "######### Sentiment #########\n",
            "The average sentiment score for the hateful comments is -0.37580107526881734\n",
            "The average sentiment score for the non-hateful comments is -0.062103669724770626\n",
            "######### Toxicity #########\n",
            "The average toxicity score for the hateful comments is 0.378683618319932\n",
            "The average toxicity score for the non-hateful comments is 0.1361061258053561\n"
          ]
        }
      ],
      "source": [
        "hateful_sentiment = hateful_profile.view().to_pandas()['udf/sentiment_nltk:distribution/mean'][0]\n",
        "non_hateful_sentiment = non_hateful_profile.view().to_pandas()['udf/sentiment_nltk:distribution/mean'][0]\n",
        "\n",
        "hateful_toxicity = hateful_profile.view().to_pandas()['udf/toxicity:distribution/mean'][0]\n",
        "non_hateful_toxicity = non_hateful_profile.view().to_pandas()['udf/toxicity:distribution/mean'][0]\n",
        "\n",
        "print(\"######### Sentiment #########\")\n",
        "print(f\"The average sentiment score for the hateful comments is {hateful_sentiment}\")\n",
        "print(f\"The average sentiment score for the non-hateful comments is {non_hateful_sentiment}\")\n",
        "\n",
        "print(\"######### Toxicity #########\")\n",
        "print(f\"The average toxicity score for the hateful comments is {hateful_toxicity}\")\n",
        "print(f\"The average toxicity score for the non-hateful comments is {non_hateful_toxicity}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langkit-vSL8Mxyz-py3.8",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
